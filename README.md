# RLHF-Fine-Tuning-with-RLOO ğŸš€

### Description ğŸ“

Welcome to RLHF-Fine-Tuning-with-RLOO! This notebook demonstrates an end-to-end framework for fine-tuning a Large Language Model (LLM) using Reinforcement Learning from Human Feedback (RLHF) and RLOO technique. ğŸ§ âœ¨

The RLHF pipeline consist of 3 phases -

1. Supervised Fine-tuning
2. Reward Model
3. Fine-Tuning with Reinforcement learning

This notebook only focuses on the Second and Third phase using _RLOO_

### Features ğŸŒŸ

- **FREE Reward Model guide:** The notebook trains a Reward Model which is later used to fine tune the base model using RLOO, so this notebook comes with a free Reward Model training guide ;-)
- **Data Preprocessing:** Believe me or not, this is the most important part! If your dataset isn't in the correct format, you can say bye-bye to the RLHF pipeine (it will give you tonnes of errors), but worry not, this notebook does the preprocessing for you!
- **Reinforcement Learning with RLOO:** The notebook uses the RLOO by TRL to fine tune the LLM (GPT2 in this case, but you can use any LLM you wish!)
- **Customizable Architecture:** Compatible with any LLM architecture â€“ plug-and-play! (personally tried on Llama2 7b chat, DistilBERT)

### Accomplishments ğŸ¯

- Fine-tuned an LLM to improve response quality based on user preferences.
- Trained Reward Model
- Demonstrated the integration of RLOO for more stable training outcomes.
- Created a lightweight, memory-efficient pipeline for rapid experimentation.

### Prerequisites ğŸ“¦

- Python 3.x
- A suitable deep learning framework (PyTorch or TensorFlow)
- TRL is version sensitive so for this notebook `pip install trl==0.12.1`
- A powerful GPU setup for faster training (recommended: NVIDIA A100 or equivalent). This notebook uses GPT2 so collab should be fine!
- Disk Space: The RLOO step takes a lot of disk space (collab may run out of disk space) so have extra storage just to be on the safe side.

### How to Use ğŸ› ï¸

1. Clone this repository to your local machine.
2. Open the notebook in your preferred IDE (e.g., Jupyter, VSCode).
3. Execute the cells step-by-step, starting with environment setup (again, trl version should be **0.12.1** for this code to run).
4. Use the provided data or customize it with your own datasets.
5. Experiment with hyperparameters for optimal results!

### Outputs ğŸ“Š

By the end of this workflow, youâ€™ll achieve:

- A fine-tuned LLM capable of providing higher-quality responses aligned with human feedback.

### Acknowledgments ğŸ™Œ

This pipeline is part of the RLHF methodology, which incorporates human feedback for creating safer and more aligned AI systems. If you encounter any issue with the code or want to know something, please feel free to contact me! Thank youuu :-)

> Crafted with â¤ï¸ by Piyush Pant (à¤ªà¤¿à¤¯à¥‚à¤· à¤ªà¤‚à¤¤)
