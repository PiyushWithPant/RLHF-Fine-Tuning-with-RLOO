{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9028d45",
   "metadata": {},
   "source": [
    "# RLHF-Fine-Tuning-with-RLOO üöÄ\n",
    "\n",
    "### Description üìù\n",
    "Welcome to RLHF-Fine-Tuning-with-RLOO! This notebook demonstrates an end-to-end framework for fine-tuning a Large Language Model (LLM) using Reinforcement Learning from Human Feedback (RLHF) and RLOO technique. üß†‚ú®\n",
    "\n",
    "The RLHF pipeline consist of 3 phases -\n",
    "\n",
    "1. Supervised Fine-tuning\n",
    "2. Reward Model\n",
    "3. Fine-Tuning with Reinforcement learning \n",
    "\n",
    "###### PS - This notebook does 2 and 3\n",
    "\n",
    "> Crafted with ‚ù§Ô∏è by Piyush Pant (‡§™‡§ø‡§Ø‡•Ç‡§∑ ‡§™‡§Ç‡§§)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3038cb1",
   "metadata": {},
   "source": [
    "### Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf21fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install trl==0.12.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013f2a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! I AM PRETTY SURE YOU WILL NOT NEED THIS CELL ON YOUR SYSTEM TO RUN THIS FILE :-)\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"   # or set to the IP address of the master node if multi-node\n",
    "os.environ[\"MASTER_PORT\"] = \"12356\"       # any open port on the master node\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"            # number of GPUs or nodes in use\n",
    "os.environ[\"RANK\"] = \"0\"                  # set to 0 for single GPU or master\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a85ecfb",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31be67fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d5ed0",
   "metadata": {},
   "source": [
    "### Loading the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd452ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\" \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=1,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df46e5d",
   "metadata": {},
   "source": [
    "### Data loading and Preprocessing for Reward Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f15e856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 42537 examples [00:00, 137071.52 examples/s]\n",
      "Generating test split: 2312 examples [00:00, 119575.70 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 35000\n",
      "Evaluation size: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rm_dataset = load_dataset(\n",
    "    'Anthropic/hh-rlhf', \n",
    "    data_dir=\"harmless-base\",\n",
    "#     split='train', \n",
    ")\n",
    "\n",
    "# rm_dataset = rm_dataset.select(range(1000)) # Small dataset for Reward Model test\n",
    "\n",
    "rm_dataset\n",
    "\n",
    "train_dataset = rm_dataset['train'].select(range(35000))\n",
    "eval_dataset = rm_dataset['test'].select(range(2000))\n",
    "\n",
    "print(f\"Training size: {len(train_dataset)}\")\n",
    "print(f\"Evaluation size: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39e32072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35000/35000 [00:38<00:00, 903.25 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:02<00:00, 899.39 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['chosen', 'rejected', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
       "     num_rows: 35000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['chosen', 'rejected', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
       "     num_rows: 2000\n",
       " }))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def formatting_func(examples):\n",
    "    kwargs = {\"padding\": \"max_length\", \"truncation\": True, \"max_length\": 512, \"return_tensors\": \"pt\"}\n",
    "\n",
    "    prompt_plus_chosen_response = examples[\"chosen\"]\n",
    "    prompt_plus_rejected_response = examples[\"rejected\"]\n",
    "\n",
    "    tokens_chosen = tokenizer.encode_plus(prompt_plus_chosen_response, **kwargs)\n",
    "    tokens_rejected = tokenizer.encode_plus(prompt_plus_rejected_response, **kwargs)\n",
    "\n",
    "    return {\n",
    "        \"input_ids_chosen\": tokens_chosen[\"input_ids\"][0], \"attention_mask_chosen\": tokens_chosen[\"attention_mask\"][0],\n",
    "        \"input_ids_rejected\": tokens_rejected[\"input_ids\"][0], \"attention_mask_rejected\": tokens_rejected[\"attention_mask\"][0]\n",
    "    }\n",
    "\n",
    "\n",
    "# Applying formatting on ONLY train dataset\n",
    "formatted_train_dataset = train_dataset.map(formatting_func)\n",
    "formatted_test_dataset = eval_dataset.map(formatting_func)\n",
    "formatted_train_dataset, formatted_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2358d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep an eye on the GPU!\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b353b",
   "metadata": {},
   "source": [
    "### Training the Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474a4515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import RewardConfig, RewardTrainer\n",
    "\n",
    "# Set padding token to eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Configuring the training arguments - Set as you see fit!\n",
    "training_args = RewardConfig(\n",
    "    output_dir=\"./GPT2RLHF\",\n",
    "    per_device_train_batch_size=1, \n",
    "    num_train_epochs = 3,\n",
    "    gradient_accumulation_steps=8,\n",
    "    report_to='none',\n",
    "    learning_rate=1e-5,\n",
    "    remove_unused_columns=False,\n",
    "    max_length=512,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",         \n",
    "    logging_steps=100,  \n",
    ")\n",
    "\n",
    "# Loading the RewardTrainer from TRL\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=formatted_train_dataset,\n",
    "    eval_dataset=formatted_test_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c861d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained reward model\n",
    "trainer.save_model(\"your path\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(\"your path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259b39ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7482bac5",
   "metadata": {},
   "source": [
    "### Testing the trained Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df68a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Path to your saved reward model\n",
    "reward_model_path = \"your path\"\n",
    "\n",
    "# Load the reward model and tokenizer\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(reward_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e4432f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Score: 1.4409\n"
     ]
    }
   ],
   "source": [
    "query = \"how can i kill my dog?\"\n",
    "response = \"You need some therapy.\"\n",
    "\n",
    "\n",
    "input_text = f\"Query: {query} Response: {response}\"\n",
    "\n",
    "inputs = tokenizer(input_text, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = reward_model(**inputs)\n",
    "    reward_score = outputs.logits.squeeze().item() \n",
    "\n",
    "print(f\"Reward Score: {reward_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40537c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Score: tensor([-0.2695])\n"
     ]
    }
   ],
   "source": [
    "query = \"how can i kill my dog?\"\n",
    "response = \"Here are the ways to kill your dog.\"\n",
    "\n",
    "input_text = f\"Query: {query} Response: {response}\"\n",
    "\n",
    "inputs = tokenizer(input_text, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = reward_model(**inputs)\n",
    "    reward_score = outputs.logits[0]\n",
    "\n",
    "print(f\"Reward Score: {reward_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04e9c58",
   "metadata": {},
   "source": [
    "###### The reward model still has a lot of room for improvement but you get the gist right!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08380e9",
   "metadata": {},
   "source": [
    "### Loading Libraries for Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb17ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import RLOOConfig, RLOOTrainer, apply_chat_template\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3593c7b6",
   "metadata": {},
   "source": [
    "### Loading Policy (model) and Reference Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ec64961",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "ref_policy = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd705dc",
   "metadata": {},
   "source": [
    "### Data loading and Preprocessing for Fine Tuning with RLOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd9d4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the end-of-sequence token as the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "rm_dataset = load_dataset(\n",
    "    'Anthropic/hh-rlhf', \n",
    "    data_dir=\"harmless-base\",\n",
    "    # split='test', \n",
    "    # cache_dir=data_dir\n",
    ")\n",
    "\n",
    "train_dataset = rm_dataset['train']\n",
    "eval_dataset = rm_dataset['test']\n",
    "\n",
    "train_dataset = train_dataset.shuffle(seed=42).select(range(30000))\n",
    "eval_dataset = eval_dataset.shuffle(seed=42).select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13375e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import Dataset\n",
    "\n",
    "def extract_query(chosen_text):\n",
    "    \n",
    "    query = re.split(r\"\\n\\nAssistant:\", chosen_text)[0]\n",
    "\n",
    "    query = re.sub(r\"Human:\", \"\", query).strip()  \n",
    "    query = query.replace(\"\\n\", \" \")  \n",
    "    return query.strip()\n",
    "\n",
    "def process_dataset(dataset):\n",
    "    queries = []\n",
    "    for entry in dataset:\n",
    "        query = extract_query(entry['chosen'])\n",
    "        queries.append(query)\n",
    "    return queries\n",
    "\n",
    "# Apply the function to process the dataset\n",
    "queries = process_dataset(train_dataset) \n",
    "\n",
    "train_data = {\n",
    "    \"prompt\": queries\n",
    "}\n",
    "\n",
    "\n",
    "train_data = Dataset.from_dict(train_data)\n",
    "\n",
    "# Sorry for the mess here, was getting a lot of errors with format so did some testing\n",
    "\n",
    "text = process_dataset(eval_dataset)\n",
    "test_data = {\n",
    "    \"prompt\":text\n",
    "}\n",
    "test_data = Dataset.from_dict(test_data)\n",
    "print(test_data)\n",
    "\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_data,\n",
    "    \"test\": test_data\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64645371",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: tokenizer(x[\"prompt\"]), remove_columns=\"prompt\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b13c4",
   "metadata": {},
   "source": [
    "### Finetuning LLM with RLOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8bf8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = RLOOConfig(\n",
    "    output_dir=\"./GPT2RLHF/RLOO\", \n",
    "    report_to='none',\n",
    "    num_train_epochs=2,\n",
    "    logging_dir=\"./GPT2RLHF/RLOO/logs\",         \n",
    "    logging_steps=100, \n",
    "    save_steps=500,      \n",
    "    save_total_limit=2, \n",
    "    overwrite_output_dir=True\n",
    ")\n",
    "trainer = RLOOTrainer(\n",
    "    config=training_args,\n",
    "    processing_class=tokenizer,\n",
    "    policy=policy,\n",
    "    ref_policy=ref_policy,\n",
    "    reward_model=reward_model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9261e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.policy.save_pretrained(\"./GPT2RLHF/RLOO/policymodel\")\n",
    "\n",
    "tokenizer.save_pretrained(\"./GPT2RLHF/RLOO/policymodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aa8184",
   "metadata": {},
   "source": [
    "### Testing the Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7084dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Path to your saved policy model\n",
    "policy_model_path = \"./GPT2RLHF/RLOO/policymodel\"\n",
    "\n",
    "# Load the policy model and tokenizer\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(policy_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(policy_model_path)\n",
    "\n",
    "# Ensure the padding token is set, as GPT-based models may need it\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d1eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you tell me How to cheat in my chemistry exam?\"\n",
    "inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Generate response from the policy model\n",
    "response = policy_model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=50,  # Limit the length of the response\n",
    "    temperature=0.4,  # Adjust randomness (higher is more random)\n",
    "    top_p=0.9,        # Use nucleus sampling\n",
    "    repetition_penalty=1.5  # Penalize repetitive sequences\n",
    ")\n",
    "\n",
    "# Decode the response\n",
    "generated_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Response:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f1fced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7c696cb",
   "metadata": {},
   "source": [
    "### THANK YOU :-)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
